# Classificador de Sentimentos com Naive Bayes (Feito do Zero)

Este projeto implementa, do zero, um classificador de sentimentos baseado em **Naive Bayes**, utilizando uma base de dados do Kaggle.
O modelo √© capaz de analisar textos curtos e classific√°-los em **6 emo√ß√µes** distintas:

* üò¢ Sadness
* üòÑ Joy
* ‚ù§Ô∏è Love
* üò° Anger
* üò® Fear
* üò≤ Surprise

O foco principal √© **educacional**, mostrando como funciona um classificador probabil√≠stico sem o uso de bibliotecas de machine learning como scikit-learn.

---

# Objetivo

O objetivo deste reposit√≥rio √© demonstrar, de maneira clara e did√°tica, como construir um modelo de classifica√ß√£o de sentimentos usando apenas:

* Python puro
* Pr√©-processamento manual
* Contagem de frequ√™ncias
* Probabilidades do Naive Bayes
* Organiza√ß√£o em classes e arquitetura modular

Ideal para quem est√° aprendendo NLP e quer entender **como as coisas funcionam por baixo dos panos**.

---

# Como executar

### 1 - Instalar depend√™ncias

```
pip install openpyxl
```

### 2 Rode o programa:

```
python main.py
```

Voc√™ ver√° o menu:

```
===== Sentiment Analysis =====
1 - Treinar modelo
2 - Ver acur√°cia do modelo
3 - Usar modelo pr√©-treinado
```

---

# üîß Como o projeto funciona

Abaixo est√° uma explica√ß√£o detalhada do pipeline.

---

## **1. Carregamento dos dados**

Utilizamos a classe `Excel` para ler o arquivo emotions.xlsx:

```python
data = Excel("emotions.xlsx").get_data()
```

Cada linha do Excel vira um objeto `Node` contendo:

* a frase
* a emo√ß√£o correspondente

---

## **2. Divis√£o em treino e teste**

Os dados s√£o embaralhados e divididos conforme o padr√£o:

* **80% treino**
* **20% teste**

```python
trainNodes, testNodes = do_train_test_split(nodes)
```

---

## **3. Pr√©-processamento**

Cada frase passa por:

1. **lowercase**
2. **divis√£o em palavras**
3. **remo√ß√£o de stopwords**

Isso √© feito na classe `Node`:

```python
self.__phrase = phraseManipulator.to_lower(self.__phrase)
self.__phrase = phraseManipulator.split_phrase(self.__phrase)
self.__phrase = stopWordCutter.cut_stop_word_from_line(self.__phrase)
```

---

## **4. Treinamento do Naive Bayes**

O modelo percorre todas as palavras do set de treino e constr√≥i um dicion√°rio:

```
"love":  [5, 120, 80, 2, 0, 3]
          ‚Üë   ‚Üë   ‚Üë
         Sad Joy Love ...
```

Cada posi√ß√£o da lista representa a frequ√™ncia da palavra em cada emo√ß√£o.

Trecho essencial:

```python
for word in node.get_phrase():
    if word not in self.__database:
        self.__database[word] = [0] * 6
    self.__database[word][node.get_emotion()] += 1
```

Ao final, √© calculado o total de palavras de cada emo√ß√£o.

---

**Fundamentos Matem√°ticos**

O modelo usa o algoritmo **Naive Bayes**, que basicamente tenta descobrir **qual sentimento tem a maior chance de gerar as palavras da frase que o usu√°rio digitou**.

**A ideia central √©:**

> *"Para cada emo√ß√£o, veja o qu√£o prov√°vel √© que ela gere as palavras da frase. No final, escolho a emo√ß√£o com maior probabilidade."*

Para isso, usamos uma conta chamada **probabilidade condicional**.

---

## **Probabilidade de uma palavra aparecer em um sentimento**

Durante o treinamento, contamos quantas vezes cada palavra aparece em cada emo√ß√£o:
```
love ‚Üí [5, 120, 80, 2, 0, 3]
         ‚Üë   ‚Üë   ‚Üë
       Sad Joy Love ...
```
> Nesse exemplo a palava `love` apareceu 5 vezes em frases tristes, 120 em frases alegres, 80 em amorosas, etc

Com isso, calculamos:

$$
P(W|S) = \frac{\text{count}(W,S) + 1}{\text{Total}_S + \text{Total}_{\text{palavras}}}
$$

Essa √© a probabilidade da palavra **W** aparecer dentro do sentimento **S**. Usamos "+1" para evitar divis√£o por zero ‚Äî isso se chama **suaviza√ß√£o de Laplace**.

---

## **Combinando todas as palavras da frase**

Se a frase tem v√°rias palavras, calculamos a probabilidade de cada uma pertencer ao sentimento S e multiplicamos:

$$
R_S = P(W_1|S) \times P(W_2|S) \times \cdots \times P(W_n|S)
$$

Esse n√∫mero final representa o "qu√£o compat√≠vel" a frase √© com aquele sentimento.

---

## **Escolha da emo√ß√£o final**

Depois de fazer isso para:
- Sadness
- Joy  
- Love
- Anger
- Fear
- Surprise

Escolhemos a emo√ß√£o com o maior valor:

$$
\text{Emotion} = \arg\max_S (R_S)
$$

Ou seja: **a emo√ß√£o mais prov√°vel segundo o Naive Bayes**.



---

# Sobre Stemming (e por que n√£o foi usado)

Foi implementado um teste com o **PorterStemmer** da NLTK:

* Ele reduz palavras para ra√≠zes como:

  * *loving ‚Üí love*
  * *happiness ‚Üí happi*
  * *crying ‚Üí cri*

Embora funcione bem em muitos contextos, **neste modelo o stemming reduziu a acur√°cia** de:

```
68‚Äì70% (sem stemming)
‚Üì
65% (com stemming)
```

### **Por qu√™?**

Porque o PorterStemmer perde muita informa√ß√£o sem√¢ntica e gera tokens que n√£o existem no vocabul√°rio original:

```
happy ‚Üí happi
happiness ‚Üí happi
```

Como o modelo depende **exclusivamente de contagens exatas das palavras**, ele deixa de reconhecer palavras importantes, o que reduz a precis√£o.

Por isso, o projeto utiliza **palavras originais**, apenas normalizadas e com stopwords removidas.

---

# Acur√°cia obtida

Em m√©dia, o modelo obt√©m:

```
Acur√°cia: 68% ‚Äì 70%
```

Isso √© esperado para um Naive Bayes simples com foco no vi√©s educativo.

---

# üí¨ Como usar o modelo (op√ß√£o 3)

Depois de treinar ou carregar o `params.txt`, voc√™ pode digitar frases:

```
Digite um texto: I'm loving it
Sentimento detectado: Love
```

---

